{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP 543 Assignment #5\n",
    "Rung-De Chu (rc118)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Note : I used SMALL DATA SET for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "corpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/TestingDataOneLinePerDoc.txt\")\n",
    "#corpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/SmallTrainingDataOneLinePerDoc.txt\")\n",
    "\n",
    "# each entry in validLines will be a line from the text file\n",
    "validLines = corpus.filter(lambda x : 'id' in x)\n",
    "# now we transform it into a bunch of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x :(x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "# now we split the text in each (docID, text) pair into a list of words\n",
    "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "# die on some of the documents\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ',x[1]).lower().split()))\n",
    "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "# now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey (lambda a, b: a + b)\n",
    "# and get the top 20,000 words in a local array\n",
    "# each entry is a (\"word1\", count) pair\n",
    "topWords = allCounts.top (20000, lambda x : x[1])\n",
    "twentyK = sc.parallelize(range(20000))\n",
    "dictionary = twentyK.map (lambda x : (topWords[x][0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_find = [\"applicant\", \"and\", \"attack\", \"protein\", \"car\"]\n",
    "positions = {word: dictionary.lookup(word)[0] if dictionary.lookup(word) else -1 for word in word_to_find}\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of task 1 : \n",
    "using small training dataset :\n",
    "```plaintext\n",
    "print(positions)\n",
    "{'applicant': 347, 'and': 2, 'attack': 504, 'protein': 3018, 'car': 612}\n",
    "```\n",
    "\n",
    "using testing dataset :\n",
    "```plaintext\n",
    "print(positions)\n",
    "{'applicant': 604, 'and': 2, 'attack': 515, 'protein': 3681, 'car': 635}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 \n",
    "\n",
    "\n",
    "\n",
    "$LLH = \\sum_{i}(y_i{\\theta}^Tx_i - \\log(1 + e^{\\theta^Tx_i})), where y_i \\in \\{0, 1\\}, x \\in R^{20000}, \\theta \\in R^{20000}$\n",
    "\n",
    "After regularization: $ LLH = \\sum_{i}(y_i{\\theta}^Tx_i - \\log(1 + e^{\\theta^Tx_i})) + \\frac{\\beta}{2}||\\theta||_2^2 $\n",
    "\n",
    "$ \\frac{\\partial LLH}{\\partial \\theta} = \\sum_{i}(y_i x_i - \\frac{e^{\\theta^Tx_i}}{1+e^{\\theta^Tx_i}} x_i) + 2\\beta||\\theta||_2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "allDictionaryWords = dictionary.join(allWords)\n",
    "doc_pos_pair = allDictionaryWords.map(lambda x: (x[1][1], x[1][0])).groupByKey()\n",
    "docTerms = doc_pos_pair.map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "def calculate_tf(word_indices):\n",
    "    tf_vector = np.zeros(20000)  \n",
    "    for index in word_indices:  \n",
    "        tf_vector[index] += 1  \n",
    "    sum_tf = np.sum(tf_vector)  \n",
    "    return tf_vector / sum_tf if sum_tf > 0 else tf_vector  \n",
    "\n",
    "def calculate_df(docs):\n",
    "    df_vector = np.zeros(20000)\n",
    "    for word_indices in docs:\n",
    "        unique_indices = set(word_indices)\n",
    "        for index in unique_indices:\n",
    "            df_vector[index] += 1\n",
    "    return df_vector\n",
    "\n",
    "def calculate_idf(df_vector, n_docs):\n",
    "    return np.log(n_docs / (df_vector + 1))\n",
    "\n",
    "n_docs = docTerms.count()\n",
    "word_indices_docs = docTerms.map(lambda x: x[1]).collect()  \n",
    "df_vector = calculate_df(word_indices_docs)  \n",
    "idf_vector = calculate_idf(df_vector, n_docs)  \n",
    "tf_idf = docTerms.map(lambda x: (x[0], calculate_tf(x[1]) * idf_vector))\n",
    "\n",
    "# Normalize\n",
    "def normalize(data):\n",
    "    global mean, std\n",
    "    mean = data.map(lambda x: x[1]).mean()\n",
    "    std = data.map(lambda x: x[1]).stdev()\n",
    "    std = np.where(std == 0, 1, std)\n",
    "    return data.map(lambda a: (a[0], (a[1] - mean) /std))\n",
    "  \n",
    "n = tf_idf.count()\n",
    "allDocsAsNumpyArrays = normalize(tf_idf)\n",
    "\n",
    "def cal_grad(x, r, beta):\n",
    "    y_i = 1 if \"AU\" in str(x[0]) else 0\n",
    "    net_i = np.dot(r, x[1])\n",
    "    grad = -x[1] * y_i + x[1] * (np.exp(net_i) / (1 + np.exp(net_i))) + 2 * beta * r\n",
    "    return grad\n",
    "\n",
    "def cal_nllh(x, r):\n",
    "    y_i = 1 if \"AU\" in str(x[0]) else 0\n",
    "    net_i = np.dot(r, x[1])\n",
    "    nll = -y_i * net_i + np.log(1 + np.exp(net_i))\n",
    "    return nll\n",
    "\n",
    "def gd_optimize(tf_idf, beta=0.0001, max_iter=100):\n",
    "    r = np.zeros(20000)\n",
    "    delta = 1\n",
    "    lr = 1\n",
    "    loss_now = tf_idf.map(lambda x: cal_nllh(x, r)).reduce(lambda a, b: a + b) + beta * np.linalg.norm(r) ** 2\n",
    "    num_epoch = 0\n",
    "    while delta > 0.0001 and num_epoch < max_iter:\n",
    "        num_epoch += 1\n",
    "        grad = tf_idf.map(lambda x: cal_grad(x, r, beta)).reduce(lambda a, b: a + b) / tf_idf.count()\n",
    "        r -= lr * grad\n",
    "        loss_next = tf_idf.map(lambda x: cal_nllh(x, r)).reduce(lambda a, b: a + b) + beta * np.linalg.norm(r) ** 2\n",
    "        delta = abs(loss_next - loss_now)\n",
    "        print(f\"Epoch: {num_epoch}, Negative Log Likelihood: {loss_next}\")\n",
    "        lr = lr/ 2 if loss_next > loss_now else lr * 1.1\n",
    "        loss_now = loss_next\n",
    "    return r\n",
    "\n",
    "\n",
    "w = np.zeros(20000)\n",
    "w = gd_optimize(allDocsAsNumpyArrays, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dict_reverse = dictionary.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "top_50_indices_desc = np.argsort(w)[-50:][::-1]\n",
    "for idx in top_50_indices_desc:\n",
    "    print(w_dict_reverse.lookup(idx)[0], \"'s coefficient:\", w[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Term           | Coefficient      |\n",
    "|----------------|------------------|\n",
    "| applicant      | 0.4313592677577349 |\n",
    "| clr            | 0.4238193258068558 |\n",
    "| pty            | 0.4194318653850585 |\n",
    "| fcr            | 0.41223204589155593 |\n",
    "| hca            | 0.39654863901192233 |\n",
    "| alr            | 0.3823961644927237 |\n",
    "| respondent     | 0.36602988041261597 |\n",
    "| relevantly     | 0.34917705762561213 |\n",
    "| fca            | 0.3461214447536104 |\n",
    "| tribunal       | 0.3390207563746712 |\n",
    "| submissions    | 0.3386263435487898 |\n",
    "| fcafc          | 0.32447521140336827 |\n",
    "| gummow         | 0.31547952719345107 |\n",
    "| affidavit      | 0.3146678362297677 |\n",
    "| pursuant       | 0.3071781278022647 |\n",
    "| proceeding     | 0.30181605547601065 |\n",
    "| application    | 0.2980407215956911 |\n",
    "| satisfied      | 0.29590223530279197 |\n",
    "| appellant      | 0.2796192834484266 |\n",
    "| respondents    | 0.24148047156089478 |\n",
    "| submits        | 0.2362140328839448 |\n",
    "| gaudron        | 0.22605320868349993 |\n",
    "| amp            | 0.22412108459502442 |\n",
    "| costs          | 0.22284432651921957 |\n",
    "| proceedings    | 0.22184471626512264 |\n",
    "| reasons        | 0.22067288656993814 |\n",
    "| jurisdictional | 0.2128643122123014 |\n",
    "| relevant       | 0.20574439554196747 |\n",
    "| nswlr          | 0.20459562853812024 |\n",
    "| mr             | 0.2035292721402673 |\n",
    "| magistrate     | 0.20284850617512765 |\n",
    "| affidavits     | 0.20118799525813258 |\n",
    "| interlocutory  | 0.1973322594282697 |\n",
    "| notice         | 0.19438038891134007 |\n",
    "| hearing        | 0.1925708019227608 |\n",
    "| circumstances  | 0.19208673050126412 |\n",
    "| relation       | 0.19149283945333015 |\n",
    "| multicultural  | 0.18995626017913783 |\n",
    "| arguable       | 0.18827129515627228 |\n",
    "| contravention  | 0.18800941016214426 |\n",
    "| evidence       | 0.1855428995385597 |\n",
    "| honour         | 0.18255506852887857 |\n",
    "| magistrates    | 0.17449152918814753 |\n",
    "| acsr           | 0.17259076559798467 |\n",
    "| applicants     | 0.17176509004733795 |\n",
    "| orders         | 0.1680815733885168 |\n",
    "| paragraphs     | 0.16607556863724812 |\n",
    "| erred          | 0.16387846118789004 |\n",
    "| ltd            | 0.16140056274421033 |\n",
    "| contraventions | 0.16098774942057018 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "pcorpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/SmallTraingDataOneLinePerDoc.txt\")\n",
    "# each entry in validLines will be a line from the text file\n",
    "pvalidLines = pcorpus.filter(lambda x : 'id' in x)\n",
    "# now we transform it into a bunch of (docID, text) pairs\n",
    "pkeyAndText = validLines.map(lambda x :(x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "# now we split the text in each (docID, text) pair into a list of words\n",
    "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "# die on some of the documents\n",
    "pkeyAndListOfWords = pkeyAndText.map(lambda x : (str(x[0]), regex.sub(' ',x[1]).lower().split()))\n",
    "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "pallDictionaryWords = dictionary.join (allWords)\n",
    "pdoc_pos_pair = pallDictionaryWords.map (lambda x: (x[1][1], x[1][0])).groupByKey ()\n",
    "pdocTerms = pdoc_pos_pair.map(lambda x: (1 if x[0][0] == \"A\" else 0, x[1]))\n",
    "\n",
    "n_docs = pdocTerms.count()\n",
    "p_word_indices_docs = pdocTerms.map(lambda x: x[1]).collect()\n",
    "pdf_vector = calculate_df(p_word_indices_docs)\n",
    "pidf_vector = calculate_idf(pdf_vector, n_docs)\n",
    "p_tf_idf = pdocTerms.map(lambda x: (x[0], calculate_tf(x[1]) * pidf_vector))\n",
    "pallDocsAsNumpyArrays = p_tf_idf.map(lambda a: (a[0], (a[1] - mean)/ std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, r, cut =0):\n",
    "    y_true_pred = test.map(lambda x: (x[0], 1 if r.dot(x[1]) > cut else 0))\n",
    "    res = np.array(y_true_pred.collect())\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for idx in range(res.shape[0]):\n",
    "        if ((res[idx,0] == 1) and (res[idx,1] == 1)):\n",
    "            tp += 1\n",
    "        elif ((res[idx,0] == 0) and (res[idx,1] == 0)):\n",
    "            tn += 1\n",
    "        elif ((res[idx,0] == 1) and (res[idx,1] == 0)):\n",
    "            fn += 1\n",
    "        elif ((res[idx,0] == 0) and (res[idx,1] == 1)):\n",
    "            fp += 1\n",
    "            print(\"fp index:\", idx)\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp/ (tp + fp) if    (tp + fp) > 0 else 0\n",
    "    F1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(\"%d out of %d predictions correct.\" % (tp + tn, len(res)))\n",
    "    print(f\"test accuracy: {accuracy:.5f}\")\n",
    "    print(f\"recall: {recall:.5f}\")\n",
    "    print(f\"precision: {precision:.5f}\")\n",
    "    print(f\"F1 score: {F1_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(pallDocsAsNumpyArrays, w, cut = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict(pallDocsAsNumpyArrays, w, cut = 10)\n",
    "\n",
    "fp index: 96\n",
    "\n",
    "fp index: 571\n",
    "\n",
    "fp index: 2897\n",
    "\n",
    "fp index: 4401\n",
    "\n",
    "fp index: 6830\n",
    "\n",
    "fp index: 7668\n",
    "\n",
    "fp index: 9092\n",
    "\n",
    "fp index: 12371\n",
    "\n",
    "fp index: 13796\n",
    "\n",
    "fp index: 17413\n",
    "\n",
    "fp index: 17420\n",
    "\n",
    "18713 out of 18724 predictions correct.\n",
    "\n",
    "test accuracy: 0.99941\n",
    "\n",
    "recall: 1.00000\n",
    "\n",
    "precision: 0.97165\n",
    "\n",
    "F1 score: 0.98562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a lookup tool to examine the index above. The Wikipedia documents contain terms related to labor and affairs, and they share some of the top 50 listed words."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
